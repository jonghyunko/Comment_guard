{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1H56uLZ7x3-SJjwnaUTqt5jrl3Z-uTgiJ","authorship_tag":"ABX9TyOOJZxc4oi2wkucZc3N1MUn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9c42086db5254a448bf63c9dfc4b2a73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6419a0a16fce41f48aa8e50d0e62ba8e","IPY_MODEL_46fbf0ac41f54afb8d82c9fa755cf49e","IPY_MODEL_fed77f9e19034302ae1d47dbaa7a5252"],"layout":"IPY_MODEL_a1de188a2b8e4540a73882fc641fcbb6"}},"6419a0a16fce41f48aa8e50d0e62ba8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db85cd4bf13d40c686948a52e3646987","placeholder":"​","style":"IPY_MODEL_55fe163fe7c14c629154efbb84fea7bd","value":"model.safetensors: 100%"}},"46fbf0ac41f54afb8d82c9fa755cf49e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4aa8fa5562cb41b69d6ae0d8643455d4","max":368769812,"min":0,"orientation":"horizontal","style":"IPY_MODEL_194d92ff5a8645f29dae1aad5f0fc050","value":368769812}},"fed77f9e19034302ae1d47dbaa7a5252":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f47bbd1ec226460a83731e4392b3b46a","placeholder":"​","style":"IPY_MODEL_b914236bfd404e9eb5da9c25aac08287","value":" 369M/369M [00:02&lt;00:00, 81.8MB/s]"}},"a1de188a2b8e4540a73882fc641fcbb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db85cd4bf13d40c686948a52e3646987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55fe163fe7c14c629154efbb84fea7bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4aa8fa5562cb41b69d6ae0d8643455d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194d92ff5a8645f29dae1aad5f0fc050":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f47bbd1ec226460a83731e4392b3b46a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b914236bfd404e9eb5da9c25aac08287":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSKDR9QhzJty","executionInfo":{"status":"ok","timestamp":1701676137438,"user_tz":-540,"elapsed":18383,"user":{"displayName":"종현","userId":"00841649676888510360"}},"outputId":"8549518b-a65a-4d76-afce-fafd3263a178"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"eGPLwGZqznlA","executionInfo":{"status":"ok","timestamp":1701677301583,"user_tz":-540,"elapsed":2054,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")"],"metadata":{"id":"5t9X708F1vqk","executionInfo":{"status":"ok","timestamp":1701677307948,"user_tz":-540,"elapsed":789,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('/content/drive/MyDrive/NLP_commit/Comment_guard/unsmile_train_v1.0.tsv', delimiter='\\t')\n","valid_df = pd.read_csv('/content/drive/MyDrive/NLP_commit/Comment_guard/unsmile_valid_v1.0.tsv', delimiter='\\t')"],"metadata":{"id":"ZenaRWQS0sKG","executionInfo":{"status":"ok","timestamp":1701676793681,"user_tz":-540,"elapsed":405,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# '문장' 열과 레이블 생성 (0: 부정적인 문장)\n","train_sentences = train_df['문장']\n","train_labels = torch.zeros(len(train_df), dtype=torch.long)\n","\n","valid_sentences = valid_df['문장']\n","valid_labels = torch.zeros(len(valid_df), dtype=torch.long)\n"],"metadata":{"id":"KXIMlARf0tsI","executionInfo":{"status":"ok","timestamp":1701677400164,"user_tz":-540,"elapsed":5,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 문장을 토크나이저로 변환하여 패딩 적용하는 함수 정의\n","def tokenize_sentences(sentences, max_length=50):\n","    tokens = tokenizer(sentences.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","    return tokens"],"metadata":{"id":"sW_EISR31JZA","executionInfo":{"status":"ok","timestamp":1701677412548,"user_tz":-540,"elapsed":7,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 훈련 데이터와 검증 데이터 전처리\n","train_tokens = tokenize_sentences(train_sentences)\n","valid_tokens = tokenize_sentences(valid_sentences)"],"metadata":{"id":"lVUhacHd1XXM","executionInfo":{"status":"ok","timestamp":1701677429262,"user_tz":-540,"elapsed":8149,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(train_tokens)\n","print(valid_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xNsxM8414m-","executionInfo":{"status":"ok","timestamp":1701676816114,"user_tz":-540,"elapsed":12,"user":{"displayName":"종현","userId":"00841649676888510360"}},"outputId":"e516995e-c1f9-4112-805a-50b0a769e875"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[2, 0, 0,  ..., 1, 1, 1],\n","        [2, 0, 0,  ..., 1, 1, 1],\n","        [2, 0, 0,  ..., 1, 1, 1],\n","        ...,\n","        [2, 0, 0,  ..., 1, 1, 1],\n","        [2, 0, 0,  ..., 1, 1, 1],\n","        [2, 0, 0,  ..., 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])}\n","{'input_ids': tensor([[   2,    0, 7347,  ...,    1,    1,    1],\n","        [   2,    0,    0,  ...,    1,    1,    1],\n","        [   2,    0, 5782,  ...,    1,    1,    1],\n","        ...,\n","        [   2,    0,    0,  ...,    1,    1,    1],\n","        [   2,  497,    0,  ...,    1,    1,    1],\n","        [   2,    0,    0,  ...,    1,    1,    1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])}\n"]}]},{"cell_type":"code","source":["# 데이터셋 클래스 정의\n","class NegativeSentenceDataset(Dataset):\n","    def __init__(self, input_ids, labels):\n","        self.input_ids = input_ids\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.input_ids[idx], 'labels': self.labels[idx]}"],"metadata":{"id":"rKwC64pn17La","executionInfo":{"status":"ok","timestamp":1701677481314,"user_tz":-540,"elapsed":333,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# DataLoader 생성\n","train_dataset = NegativeSentenceDataset(train_tokens['input_ids'], train_labels)\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","valid_dataset = NegativeSentenceDataset(valid_tokens['input_ids'], valid_labels)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"2TrUjj3T4haZ","executionInfo":{"status":"ok","timestamp":1701677508064,"user_tz":-540,"elapsed":356,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=2)  # 이진 분류"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["9c42086db5254a448bf63c9dfc4b2a73","6419a0a16fce41f48aa8e50d0e62ba8e","46fbf0ac41f54afb8d82c9fa755cf49e","fed77f9e19034302ae1d47dbaa7a5252","a1de188a2b8e4540a73882fc641fcbb6","db85cd4bf13d40c686948a52e3646987","55fe163fe7c14c629154efbb84fea7bd","4aa8fa5562cb41b69d6ae0d8643455d4","194d92ff5a8645f29dae1aad5f0fc050","f47bbd1ec226460a83731e4392b3b46a","b914236bfd404e9eb5da9c25aac08287"]},"id":"X31ePfKH4pxF","executionInfo":{"status":"ok","timestamp":1701677549079,"user_tz":-540,"elapsed":8391,"user":{"displayName":"종현","userId":"00841649676888510360"}},"outputId":"f68fcb69-5d90-4473-e16e-245ccb120197"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c42086db5254a448bf63c9dfc4b2a73"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 옵티마이저 및 손실 함수 정의\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EevNdN34y4J","executionInfo":{"status":"ok","timestamp":1701677586325,"user_tz":-540,"elapsed":445,"user":{"displayName":"종현","userId":"00841649676888510360"}},"outputId":"47514b2c-b133-412d-d2b7-6e9188867d83"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 학습 및 평가 함수 정의\n","def train_epoch(model, dataloader, loss_fn, optimizer):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in dataloader:\n","        input_ids, labels = batch['input_ids'], batch['labels']\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, labels=labels)\n","        loss = loss_fn(outputs.logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate(model, dataloader, loss_fn):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids, labels = batch['input_ids'], batch['labels']\n","            outputs = model(input_ids, labels=labels)\n","            loss = loss_fn(outputs.logits, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return total_loss / len(dataloader), accuracy."],"metadata":{"id":"O7l56Bkl43gN","executionInfo":{"status":"ok","timestamp":1701677609350,"user_tz":-540,"elapsed":8,"user":{"displayName":"종현","userId":"00841649676888510360"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer)\n","    valid_loss, valid_accuracy = evaluate(model, valid_dataloader, loss_fn)\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"id":"3qLnILHY49uz","executionInfo":{"status":"error","timestamp":1701678229596,"user_tz":-540,"elapsed":607086,"user":{"displayName":"종현","userId":"00841649676888510360"}},"outputId":"1ff8ff48-93a8-4e30-ab4a-f9aa4f733f53"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-2ba2ae6b7d1d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-1bfb6f652b86>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}